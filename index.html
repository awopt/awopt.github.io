
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>AW-Opt</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <base href="/"> -->

        <!--FACEBOOK-->
    <meta property="og:image" content="https://aw-opt.github.com/img/awopt.png">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="682">
    <meta property="og:image:height" content="682">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://aw-opt.github.com/"/>
    <meta property="og:title" content="AW-Opt" />
    <meta property="og:description" content="Project page for AW-Opt: Learning Robotic Skills with Imitation and Reinforcement at Scale." />

        <!--TWITTER-->
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="AW-Opt" />
    <meta name="twitter:description" content="Project page for AW-Opt: Learning Robotic Skills with Imitation and Reinforcement at Scale." />
    <meta name="twitter:image" content="https://aw-opt.github.com/img/awopt.png" />


<!--     <link rel="apple-touch-icon" href="apple-touch-icon.png"> -->
  <!-- <link rel="icon" type="image/png" href="img/seal_icon.png"> -->
    <!-- Place favicon.ico in the root directory -->

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                <b>AW-Opt</b>: Learning Robotic Skills with <br> Imitation and Reinforcement Learning at Scale </br> 
                <small>
                    CoRL 2021
                </small>
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                          Yao Lu
                    </li>
                    <li>
                        <a href="http://karolhausman.github.io/">
                            Karol Hausman
                        </a>
                    </li>
                    <li>
                        <a href="http://www-clmc.usc.edu/Main/YevgenChebotar">
                          Yevgen Chebotar
                        </a>
                    </li>
                    <li>
			<a href="https://scholar.google.com/citations?user=-S_9ZRcAAAAJ&hl=en">
                          Mengyuan Yan
			</a>
                    </li>
                    <li>
                        <a href="https://evjang.com/">
                          Eric Jang
                        </a>
                    </li>
 	            <li>
                        <a href="https://scholar.google.com/citations?user=jrfFYAIAAAAJ&hl=en">
                          Alexander Herzog
                        </a>
                    </li><br>
                    <li>
                        <a href="https://tedxiao.me/">
                          Ted Xiao
                        </a>
                    </li>
                    <li>
                    	<a href="https://www.alexirpan.com/">
                          Alex Irpan
                  	</a>
                    </li>                    
                    <li>
                        <a href="https://scholar.google.ch/citations?user=Z3dxz9IAAAAJ&hl=en">
                          Mohi Khansari
                        </a>
                    </li>
                    <li>
                          Dmitry Kalashnikov
                    </li>                    
                    <li>
                        <a href="http://people.eecs.berkeley.edu/~svlevine/">
                          Sergey Levine
                        </a>
                    </li><br><br>
                    <a href="https://research.google/teams/brain/robotics/">
                    <image src="img/robotics-at-google.png" height="40px"> Robotics at Google</a> <br><br>
                </ul>
            </div>
        </div>


        <div class="row">
                <div class="col-md-4 col-md-offset-4 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <!-- li>
                            <a href="https://arxiv.org/abs/2104.08212">
                            <image src="img/mip_paper_image.jpg" height="60px">
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li -->
                        <!-- li>
                            <a href="https://youtu.be/i3uUGSko2zY">
                            <image src="img/youtube_icon.png" height="60px">
                                <h4><strong>Video</strong></h4>
                            </a>
                        </li -->
                        <!-- li>
                            <a href="https://ai.googleblog.com/2021/04/multi-task-robotic-reinforcement.html">
                            <image src="img/google-ai-blog-small.png" height="60px">
                                <h4><strong>Blogpost</strong></h4>
                            </a>
                        </li -->
                        <!-- <li>
                            <a href="https://github.com/">
                            <image src="img/github.png" height="60px">
                                <h4><strong>Code</strong></h4>
                            </a>
                        </li> -->
                    </ul>
                </div>
        </div>



        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <p style="text-align:center;">
        	    <image src="img/arm-farm.gif" class="img-responsive">
                </p>
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
Robotic skills can be learned via imitation learning (IL) using user-provided demonstrations, or via reinforcement learning (RL) using large amounts of autonomously collected experience. Both methods have complementary strengths and weaknesses: RL can reach a high level of performance, but requires exploration, which can be very time consuming and unsafe; IL does not require exploration, but only learns skills that are as good as the provided demonstrations. Can a single method combine the strengths of both approaches? A number of prior methods have aimed to address this question, proposing a variety of techniques that integrate elements of IL and RL. However, scaling up such methods to complex robotic skills that integrate diverse offline data and generalize meaningfully to real-world scenarios still presents a major challenge. In this paper, our aim is to test the scalability of prior IL + RL algorithms and devise a system based on detailed empirical experimentation that combines existing components in the most effective and scalable way. To that end, we present a series of experiments aimed at understanding the implications of each design decision, so as to develop a combined approach that can utilize demonstrations and heterogeneous prior data to attain the best performance on a range of real-world and realistic simulated robotic problems. Our complete method, which we call AW-Opt, combines elements of advantage-weighted regression and QT-Opt, providing a unified approach for integrating demonstrations and offline data for robotic manipulation.
                </p>
            </div>
        </div>



        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Video
                </h3>
                <div class="text-center">
                    <div style="position:relative;padding-top:56.25%;">
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/i3uUGSko2zY" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>
                    </div>
                </div>
            </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
            	<br>
                <h3>
                    Approach
                </h3>
                <p class="text-justify">
                To collect diverse, multi-task data at scale, we create an intuitive success-detector-based approach that allows us to quickly define new tasks and their rewards. We train a multi-task success detector using data from all the tasks and continuously update it to accommodate for distribution shifts caused by various real-world factors such as varying lighting conditions and changing background surroundings. In addition, we provide a data collection strategy to simultaneously collect data for multiple distinct tasks across multiple robots. In this strategy, we use solutions to easier tasks to effectively bootstrap learning of more complex tasks. Over time, this allows us to start training a policy for the harder tasks, and consequently, to collect better data for those tasks.
                </p>
                <p style="text-align:center;">
                    <image src="img/mt-opt.gif"  class="img-responsive" height="600px">
                </p>
                <p class="text-justify">
            	The robots generate episodes which then get labelled as success or failure for the current task. These episodes are then copied and shared across other tasks to increase the learning efficiency. The balanced batch of episodes is then sent to our multi-task RL training pipeline to train the MT-Opt policy.
            </div>
        </div>
            

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Results
                </h3>
                <p class="text-justify">
                    We train MT-Opt on a dataset of 9600 robot hours collected with 7 robots. We use offline multi-task reinforcement learning, and learn a wide variety of skills that include picking specific objects, placing them into various fixtures, aligning items on a rack, rearranging and covering objects with towels.
                </p>                
                <br>
	        <p style="text-align:center;">
		 <video id="v0" width="100%" autoplay loop muted controls>
	          <source src="img/mt-opt-grid.mp4" type="video/mp4" />
                    <!--image src="img/mt-opt-grid.gif" class="img-responsive" height="400px"-->
                </p>
                <br>
                <p class="text-justify">
                    When compared to single-task baselines, our MT-Opt system performs similarly on the tasks that have the most data (e.g. generic lifting task at 89% success), while significantly improving performance of tasks underrepresented in the dataset - 50% average success rate on rare tasks compared to 1% achieved with a single-task QT-Opt baseline and 18% success achieved with a naive multi-task QT-Opt baseline.
                </p>
	        <p style="text-align:center;">
                    <image src="img/results.png" class="img-responsive" height="400px">
                </p>
                <br>
                <p class="text-justify">
                    Using this large pre-trained model not only can we generalize to new but similar tasks in zero-shot, but also we can quickly (in ~1 day of data collection on 7 robots) fine-tune our system to new, previously unseen tasks, such as a towel-covering task shown below (resulting in 92% success rate of towel-picking and 79% success rate of object-covering), which wasn’t present in our original dataset.
                </p>
	        <p style="text-align:center;">
                    <image src="img/cover.gif" class="img-responsive" height="400px">
                </p>     
            </div>
        </div>


        
            
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly>
@article{mtopt2021arxiv,
    title={MT-OPT:
    Continuous Multi-Task Robotic Reinforcement Learning at Scale},
    author={Dmitry Kalashnkov and Jake Varley and 
            Yevgen Chebotar and Ben Swanson and 
            Rico Jonschkowski and Chelsea Finn and 
            Sergey Levine and Karol Hausman},
    journal={arXiv},
    year={2021}
}</textarea>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                The authors would like to thank Josh Weaver, Noah Brown, Khem Holden, Linda Luu and Brandon Kinman for their robot operation support. We also thank Yao Lu and Anthony Brohan for their help with distributed learning and testing infrastructure.  Tom Small for help with videos and project media. Tuna Toksoz and Garrett Peake for improving the bin reset mechanisms. Julian Ibarz, Kanishka Rao, Vikas Sindhwani and Vincent Vanhoucke for their support. Satoshi Kataoka,  Michael Ahn, and Ken Oslund for help with the underlying control stack, and the rest of the Robotics at Google team for their overall support and encouragement. All of these contributions were incredibly enabling for this project.
                    <br><br>
                The website template was borrowed from <a href="http://jonbarron.info/">Jon Barron</a>.
                </p>
            </div>
        </div>
    </div>
</body>
</html>
